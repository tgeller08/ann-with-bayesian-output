{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"VGG2BNN_Noise.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"oXAjSk8ItphR","colab_type":"text"},"source":["## VGG Network with Bayesian Output Layer"]},{"cell_type":"markdown","metadata":{"id":"mwVzGARM48wr","colab_type":"text"},"source":["## Loading VGG Network"]},{"cell_type":"markdown","metadata":{"id":"-599C_XTt75n","colab_type":"text"},"source":["Creating VGG Network Architecture"]},{"cell_type":"code","metadata":{"id":"rhUM3w9HtldC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1594684505636,"user_tz":420,"elapsed":1718,"user":{"displayName":"Tamar Geller","photoUrl":"","userId":"04951064580757149713"}},"outputId":"800e8ae2-4709-43c9-8803-87f651970392"},"source":["'''Initialize the network architecture'''\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torch.backends.cudnn as cudnn\n","\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","import os\n","import time\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","\n","size_1 = 64\n","size_2 = 64\n","size_4 = 128\n","size_5 = 128\n","size_7 = 256\n","size_8 = 256\n","size_9 = 256\n","size_10 = 256\n","size_12 = 512\n","size_13 = 512\n","size_14 = 512\n","size_15 = 512\n","size_17 = 512\n","size_18 = 512\n","size_19 = 512\n","size_20 = 512\n","\n","\n","\n","class Net(nn.Module):\n","\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        # 3 input channels for CIFAR10, VGG11 calls for 64 output channels from \n","        # the first conv layer, a batchnorm, then a ReLU\n","        self.conv1 = nn.Conv2d(3, size_1, kernel_size = 3, padding = 1)\n","        self.norm1 = nn.BatchNorm2d(64)\n","        self.relu1 = nn.ReLU()\n","        \n","        #layer 2 is a conv that produces 64 channels, same format as layer 1\n","        self.conv2 = nn.Conv2d(size_1, size_2, kernel_size = 3, padding = 1)\n","        self.norm2 = nn.BatchNorm2d(size_1)\n","        self.relu2 = nn.ReLU()\n","        \n","        #layer 3 is a pooling layer\n","        self.pool3 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n","        \n","        #layer 4 is a conv that produces 128 channels, same format as layer 1\n","        self.conv4 = nn.Conv2d(size_2, size_4, kernel_size = 3, padding = 1)\n","        self.norm4 = nn.BatchNorm2d(size_4)\n","        self.relu4 = nn.ReLU()\n","        \n","        #layer 5 is a conv that produces 128 channels, same format as layer 1\n","        self.conv5 = nn.Conv2d(size_4, size_5, kernel_size = 3, padding = 1)\n","        self.norm5 = nn.BatchNorm2d(size_5)\n","        self.relu5 = nn.ReLU()\n","        \n","        #layer 6 is a pooling layer\n","        self.pool6 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n","        \n","        #layer 7 is a conv that produces 256 channels, same format as layer 1\n","        self.conv7 = nn.Conv2d(size_5, size_7, kernel_size = 3, padding = 1)\n","        self.norm7 = nn.BatchNorm2d(size_7)\n","        self.relu7 = nn.ReLU()\n","        \n","        #layer 8 is a conv that produces 256 channels, same format as layer 1\n","        self.conv8 = nn.Conv2d(size_7, size_8, kernel_size = 3, padding = 1)\n","        self.norm8 = nn.BatchNorm2d(size_8)\n","        self.relu8 = nn.ReLU()\n","        \n","        #layer 9 is a conv that produces 256 channels, same format as layer 1\n","        self.conv9 = nn.Conv2d(size_8, size_9, kernel_size = 3, padding = 1)\n","        self.norm9 = nn.BatchNorm2d(size_9)\n","        self.relu9 = nn.ReLU()\n","        \n","        #layer 10 is a conv that produces 256 channels, same format as layer 1\n","        self.conv10 = nn.Conv2d(size_9, size_10, kernel_size = 3, padding = 1)\n","        self.norm10 = nn.BatchNorm2d(size_10)\n","        self.relu10 = nn.ReLU()\n","        \n","        #layer 11 is a pooling layer\n","        self.pool11 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n","        \n","        #layer 12 is a conv that produces 512 channels, same format as layer 1\n","        self.conv12 = nn.Conv2d(size_10, size_12, kernel_size = 3, padding = 1)\n","        self.norm12 = nn.BatchNorm2d(size_12)\n","        self.relu12 = nn.ReLU()\n","        \n","        #layer 13 is a conv that produces 512 channels, same format as layer 1\n","        self.conv13 = nn.Conv2d(size_12, size_13, kernel_size = 3, padding = 1)\n","        self.norm13 = nn.BatchNorm2d(size_13)\n","        self.relu13 = nn.ReLU()\n","        \n","        #layer 14 is a conv that produces 512 channels, same format as layer 1\n","        self.conv14 = nn.Conv2d(size_13, size_14, kernel_size = 3, padding = 1)\n","        self.norm14 = nn.BatchNorm2d(size_14)\n","        self.relu14 = nn.ReLU()\n","        \n","        #layer 15 is a conv that produces 512 channels, same format as layer 1\n","        self.conv15 = nn.Conv2d(size_14, size_15, kernel_size = 3, padding = 1)\n","        self.norm15 = nn.BatchNorm2d(size_15)\n","        self.relu15 = nn.ReLU()\n","        \n","        #layer 16 is a pooling layer\n","        self.pool16 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n","        \n","        #layer 17 is a conv that produces 512 channels, same format as layer 1\n","        self.conv17 = nn.Conv2d(size_15, size_17, kernel_size = 3, padding = 1)\n","        self.norm17 = nn.BatchNorm2d(size_17)\n","        self.relu17 = nn.ReLU()\n","        \n","        #layer 18 is a conv that produces 512 channels, same format as layer 1\n","        self.conv18 = nn.Conv2d(size_17, size_18, kernel_size = 3, padding = 1)\n","        self.norm18 = nn.BatchNorm2d(size_18)\n","        self.relu18 = nn.ReLU()\n","        \n","        #layer 19 is a conv that produces 512 channels, same format as layer 1\n","        self.conv19 = nn.Conv2d(size_18, size_19, kernel_size = 3, padding = 1)\n","        self.norm19 = nn.BatchNorm2d(size_19)\n","        self.relu19 = nn.ReLU()\n","        \n","        #layer 20 is a conv that produces 512 channels, same format as layer 1\n","        self.conv20 = nn.Conv2d(size_19, size_20, kernel_size = 3, padding = 1)\n","        self.norm20 = nn.BatchNorm2d(size_20)\n","        self.relu20 = nn.ReLU()\n","        \n","        #layer 21 is a pooling layer\n","        self.pool21 = nn.MaxPool2d(kernel_size = 2, stride = 2)\n","        \n","        #layer 22 is an average pooling layer\n","        self.pool22 = nn.AvgPool2d(kernel_size=1, stride=1)\n","        \n","        #layer 23 is a fully connected layer\n","        self.full23 = nn.Linear(size_20, 10)\n","        \n","        \n","\n","    def forward(self, x0):\n","        x1 = self.conv1(x0)\n","        x1 = self.norm1(x1)\n","        x1 = self.relu1(x1)\n","        \n","        x2 = self.conv2(x1)\n","        x2 = self.norm2(x2)\n","        x2 = self.relu2(x2)\n","        \n","        x3 = self.pool3(x2)\n","        \n","        x4 = self.conv4(x3)\n","        x4 = self.norm4(x4)\n","        x4 = self.relu4(x4)\n","        \n","        x5 = self.conv5(x4)\n","        x5 = self.norm5(x5)\n","        x5 = self.relu5(x5)\n","        \n","        x6 = self.pool6(x5)\n","        \n","        x7 = self.conv7(x6)\n","        x7 = self.norm7(x7)\n","        x7 = self.relu7(x7)\n","        \n","        x8 = self.conv8(x7)\n","        x8 = self.norm8(x8)\n","        x8 = self.relu8(x8)\n","        \n","        x9 = self.conv9(x8)\n","        x9 = self.norm9(x9)\n","        x9 = self.relu9(x9)\n","        \n","        x10 = self.conv10(x9)\n","        x10 = self.norm10(x10)\n","        x10 = self.relu10(x10)\n","        \n","        x11 = self.pool11(x10)\n","        \n","        x12 = self.conv12(x11)\n","        x12 = self.norm12(x12)\n","        x12 = self.relu12(x12)\n","        \n","        x13 = self.conv13(x12)\n","        x13 = self.norm13(x13)\n","        x13 = self.relu13(x13)\n","        \n","        x14 = self.conv14(x13)\n","        x14 = self.norm14(x14)\n","        x14 = self.relu14(x14)\n","        \n","        x15 = self.conv15(x14)\n","        x15 = self.norm15(x15)\n","        x15 = self.relu15(x15)\n","        \n","        x16 = self.pool16(x15)\n","        \n","        x17 = self.conv17(x16)\n","        x17 = self.norm17(x17)\n","        x17 = self.relu17(x17)\n","        \n","        x18 = self.conv18(x17)\n","        x18 = self.norm18(x18)\n","        x18 = self.relu18(x18)\n","        \n","        x19 = self.conv19(x18)\n","        x19 = self.norm19(x19)\n","        x19 = self.relu19(x19)\n","        \n","        x20 = self.conv20(x19)\n","        x20 = self.norm20(x20)\n","        x20 = self.relu20(x20)\n","       \n","        x21 = self.pool21(x20)\n","        \n","        x22 = self.pool22(x21)\n","        \n","        x22 = x22.view(x20.size(0), -1)\n","        x23 = self.full23(x22)\n","        \n","        #return the activations from each layer as well as the output\n","        output = x23\n","        activations = [x1, x2, x3, x4, x5, x6, x7, x8, x9, x10, x11, x12, x13, x14, x15, x16, x17, x18, x19, x20, x21]\n","        return output, activations\n","\n","def test():\n","    net = Net()\n","    #net.eval()\n","    y, x = net(torch.randn(1,3,32,32))\n","    print(y.size())\n","\n","test()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["torch.Size([1, 10])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VkdPu2P-uHSV","colab_type":"text"},"source":["Loading CIFAR-10 Dataset"]},{"cell_type":"code","metadata":{"id":"5oTkF-hcuPiQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1594684507643,"user_tz":420,"elapsed":3717,"user":{"displayName":"Tamar Geller","photoUrl":"","userId":"04951064580757149713"}},"outputId":"aa075fd9-1208-49d7-f996-e56d508d1ea5"},"source":["#device = 'cuda'\n","best_acc = 0  # best test accuracy\n","num_epochs = 30\n","num_layers = 22\n","\n","# Data\n","print('==> Preparing data..')\n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n","])\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=2)\n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n","\n","classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["==> Preparing data..\n","Files already downloaded and verified\n","Files already downloaded and verified\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3jo1aJRpuU6g","colab_type":"text"},"source":["Loading Pretrained Model"]},{"cell_type":"code","metadata":{"id":"ocZzGJnxuWYk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":68},"executionInfo":{"status":"ok","timestamp":1594684508071,"user_tz":420,"elapsed":4137,"user":{"displayName":"Tamar Geller","photoUrl":"","userId":"04951064580757149713"}},"outputId":"91fbe016-397d-4fa5-8509-869115ef4505"},"source":["print('==> Building model..')\n","device = 'cpu'\n","net = Net()\n","net = net.to(device)\n","net = torch.nn.DataParallel(net)\n","cudnn.benchmark = True\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","TRAINED_MODEL = F\"/content/gdrive/My Drive/Research/vgg19_baseline.pth\"\n","\n","state_dict = torch.load(TRAINED_MODEL, map_location=torch.device('cpu'))\n","from collections import OrderedDict\n","new_state_dict = OrderedDict()\n","for k, v in state_dict.items():\n","    name = k[7:] # remove `module.`\n","    new_state_dict[name] = v\n","# load params comment\n","net.load_state_dict(state_dict)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["==> Building model..\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"WdIB5zJByWEu","colab_type":"text"},"source":["## Creatings Activations Datasets"]},{"cell_type":"markdown","metadata":{"id":"44FW9RPuv0zw","colab_type":"text"},"source":["Collecting 20th Layer Activations and creating new dataset"]},{"cell_type":"code","metadata":{"id":"Y9Pecuahv5LQ","colab_type":"code","colab":{}},"source":["def collect_activations(model, device, dataloader, target_layer_idx):\n","  model.eval()\n","  batch_activations = []\n","  batch_targets = []\n","  \n","  for batch_idx, (inputs, targets) in enumerate(dataloader):\n","    torch.cuda.empty_cache()\n","    #if batch_idx < no_batches:\n","      #inputs, targets = inputs.to(device), targets.to(device)\n","      #inputs.requires_grad = True\n","\n","      # Collect activations\n","    with torch.no_grad():\n","      outputs, activations = net(inputs)\n","      batch_activations.append(activations[target_layer_idx - 1])\n","      batch_targets.append(targets)\n","\n","\n","  return batch_activations, batch_targets"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cqw8aYq7v7vJ","colab_type":"code","colab":{}},"source":["def create_dataset(batch_size, num_batches, net, device, dataloader, target_layer_idx):\n","  batch_activations, batch_targets = collect_activations(net, device, dataloader , target_layer_idx)\n","  dataset = []\n","  targets = []\n","  activations = []\n","  for i in range(num_batches):\n","    for j in range(batch_size):\n","      activations.append(batch_activations[i][j])\n","      targets.append(batch_targets[i][j])\n","  dataset = list(zip(activations,targets))\n","  return dataset"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SG-iRD39zyw7","colab_type":"text"},"source":["## Collecting Noisy Activation Datasets"]},{"cell_type":"code","metadata":{"id":"V4Xqhjfsz5li","colab_type":"code","colab":{}},"source":["def collect_activations(model, device, dataloader, target_layer_idx, m, sigma):\n","  model.eval()\n","  batch_activations = []\n","  batch_targets = []\n","  dataset = []\n","  \n","  for batch_idx, (inputs, targets) in enumerate(dataloader):\n","    torch.cuda.empty_cache()\n","    with torch.no_grad():\n","      if (sigma >0):\n","        inputs += torch.normal(m, sigma, size=(100,3,32,32))\n","      outputs, activations = net(inputs)\n","      for j in range(len(targets)):\n","        batch_activations.append(activations[target_layer_idx - 1][j])\n","        batch_targets.append(targets[j])\n","  dataset = list(zip(batch_activations,batch_targets))\n","\n","  return dataset"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"789k4U2If0RH","colab_type":"text"},"source":["Collecting Noisy Activations in Correct/Incorrect Datasets"]},{"cell_type":"code","metadata":{"id":"8jQFh0sef4jS","colab_type":"code","colab":{}},"source":["def createPositiveNegativeDatasets(model, device, dataloader, target_layer_idx, m, sigma):\n","  model.eval()\n","  batch_activations_c = []\n","  batch_activations_f = []\n","  batch_targets_c = []\n","  batch_targets_f = []\n","  dataset_c = []\n","  dataset_f = []\n","  \n","  for batch_idx, (inputs, targets) in enumerate(dataloader):\n","    torch.cuda.empty_cache()\n","    with torch.no_grad():\n","      if (sigma >0):\n","        inputs += torch.normal(m, sigma, size=(100,3,32,32))\n","      outputs, activations = net(inputs)\n","      _, predicted = torch.max(outputs.data, 1)\n","      predicted = predicted.to(device)\n","      for j in range(len(predicted)):\n","        if (predicted[j] != targets[j]):\n","            batch_activations_f.append(activations[target_layer_idx - 1][j])\n","            batch_targets_f.append(targets[j])\n","        else:\n","          batch_activations_c.append(activations[target_layer_idx - 1][j])\n","          batch_targets_c.append(targets[j])\n","\n","  dataset_c = list(zip(batch_activations_c,batch_targets_c))\n","  dataset_f = list(zip(batch_activations_f,batch_targets_f))\n","\n","  return dataset_c, dataset_f"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FAPR3wiFoV9c","colab_type":"text"},"source":["## Loading Bayesian Neural Network"]},{"cell_type":"code","metadata":{"id":"0OcqXOYU-WQq","colab_type":"code","colab":{}},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import torchvision\n","from torchvision import datasets, transforms\n","from torch.utils.data import DataLoader\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib import colors\n","from IPython import display\n","import os\n","from PIL import Image\n","from torch.utils.data.dataset import Dataset\n","\n","#!pip install pillow\n","#from scipy.misc import imread\n","\n","%matplotlib inline\n","\n","class NN(nn.Module):\n","    \n","    def __init__(self, input_size, hidden_size, output_size):\n","        super(NN, self).__init__()\n","        self.fc1 = nn.Linear(input_size, hidden_size)\n","        self.out = nn.Linear(hidden_size, output_size)\n","        \n","    def forward(self, x):\n","        output = self.fc1(x)\n","        output = F.relu(output)\n","        output = self.out(output)\n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9WVRHDbJuig6","colab_type":"text"},"source":["Initializing Bayesian Neural Network Architecture"]},{"cell_type":"code","metadata":{"id":"ZzW94sHRvJZF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"executionInfo":{"status":"ok","timestamp":1594684511038,"user_tz":420,"elapsed":7078,"user":{"displayName":"Tamar Geller","photoUrl":"","userId":"04951064580757149713"}},"outputId":"fae936f9-6a09-4a67-e127-92b957790a93"},"source":["!pip3 install pyro-ppl\n","import pyro\n","from pyro.distributions import Normal, Categorical\n","from pyro.infer import SVI, Trace_ELBO\n","from pyro.optim import Adam\n","\n","log_softmax = nn.LogSoftmax(dim=1)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: pyro-ppl in /usr/local/lib/python3.6/dist-packages (1.3.1)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (3.2.1)\n","Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.18.5)\n","Requirement already satisfied: pyro-api>=0.1.1 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (0.1.2)\n","Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (1.5.1+cu101)\n","Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.6/dist-packages (from pyro-ppl) (4.41.1)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.4.0->pyro-ppl) (0.16.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bOEvQiJMu9cQ","colab_type":"code","colab":{}},"source":["bnet = NN(512*2*2,1024,10)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3VXBBfubuxQ8","colab_type":"code","colab":{}},"source":["def model(x_data, y_data):\n","    \n","    fc1w_prior = Normal(loc=torch.zeros_like(bnet.fc1.weight), scale=torch.ones_like(bnet.fc1.weight))\n","    fc1b_prior = Normal(loc=torch.zeros_like(bnet.fc1.bias), scale=torch.ones_like(bnet.fc1.bias))\n","    \n","    outw_prior = Normal(loc=torch.zeros_like(bnet.out.weight), scale=torch.ones_like(bnet.out.weight))\n","    outb_prior = Normal(loc=torch.zeros_like(bnet.out.bias), scale=torch.ones_like(bnet.out.bias))\n","    \n","    priors = {'fc1.weight': fc1w_prior, 'fc1.bias': fc1b_prior,  'out.weight': outw_prior, 'out.bias': outb_prior}\n","    # lift module parameters to random variables sampled from the priors\n","    lifted_module = pyro.random_module(\"module\", bnet, priors)\n","    # sample a regressor (which also samples w and b)\n","    lifted_reg_model = lifted_module()\n","    \n","    lhat = log_softmax(lifted_reg_model(x_data))\n","    \n","    pyro.sample(\"obs\", Categorical(logits=lhat), obs=y_data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1o_LPWnQu2AE","colab_type":"code","colab":{}},"source":["softplus = torch.nn.Softplus()\n","\n","def guide(x_data, y_data):\n","    \n","    # First layer weight distribution priors\n","    fc1w_mu = torch.randn_like(bnet.fc1.weight)\n","    fc1w_sigma = torch.randn_like(bnet.fc1.weight)\n","    fc1w_mu_param = pyro.param(\"fc1w_mu\", fc1w_mu)\n","    fc1w_sigma_param = softplus(pyro.param(\"fc1w_sigma\", fc1w_sigma))\n","    fc1w_prior = Normal(loc=fc1w_mu_param, scale=fc1w_sigma_param)\n","    # First layer bias distribution priors\n","    fc1b_mu = torch.randn_like(bnet.fc1.bias)\n","    fc1b_sigma = torch.randn_like(bnet.fc1.bias)\n","    fc1b_mu_param = pyro.param(\"fc1b_mu\", fc1b_mu)\n","    fc1b_sigma_param = softplus(pyro.param(\"fc1b_sigma\", fc1b_sigma))\n","    fc1b_prior = Normal(loc=fc1b_mu_param, scale=fc1b_sigma_param)\n","    # Output layer weight distribution priors\n","    outw_mu = torch.randn_like(bnet.out.weight)\n","    outw_sigma = torch.randn_like(bnet.out.weight)\n","    outw_mu_param = pyro.param(\"outw_mu\", outw_mu)\n","    outw_sigma_param = softplus(pyro.param(\"outw_sigma\", outw_sigma))\n","    outw_prior = Normal(loc=outw_mu_param, scale=outw_sigma_param).independent(1)\n","    # Output layer bias distribution priors\n","    outb_mu = torch.randn_like(bnet.out.bias)\n","    outb_sigma = torch.randn_like(bnet.out.bias)\n","    outb_mu_param = pyro.param(\"outb_mu\", outb_mu)\n","    outb_sigma_param = softplus(pyro.param(\"outb_sigma\", outb_sigma))\n","    outb_prior = Normal(loc=outb_mu_param, scale=outb_sigma_param)\n","    priors = {'fc1.weight': fc1w_prior, 'fc1.bias': fc1b_prior, 'out.weight': outw_prior, 'out.bias': outb_prior}\n","    \n","    lifted_module = pyro.random_module(\"module\", bnet, priors)\n","    \n","    return lifted_module()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WYRlk148u88q","colab_type":"code","colab":{}},"source":["optim = Adam({\"lr\": 0.01})\n","svi = SVI(model, guide, optim, loss=Trace_ELBO())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x7oMUyFvvagB","colab_type":"text"},"source":["Train Bayesian Neural Network "]},{"cell_type":"code","metadata":{"id":"4cr8V5MrvRWQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":190},"executionInfo":{"status":"ok","timestamp":1594685118168,"user_tz":420,"elapsed":614184,"user":{"displayName":"Tamar Geller","photoUrl":"","userId":"04951064580757149713"}},"outputId":"3d63f993-aa26-4b72-808b-4234343e9fc2"},"source":["#load dataset\n","MODEL_PATH = F\"/content/gdrive/My Drive/Research/trainset_activaions\"\n","trainset_activations = torch.load(MODEL_PATH)\n","print('train set saved. len: ', len(trainset_activations))\n","print('train path: ', MODEL_PATH)\n","trainloader = torch.utils.data.DataLoader(trainset_activations, batch_size=128, shuffle=True, num_workers=2)\n","\n","#train network\n","num_iterations = 5\n","loss = 0\n","\n","for j in range(num_iterations):\n","    loss = 0\n","    for batch_id, data in enumerate(trainloader):\n","        # calculate the loss and take a gradient step\n","        #print(data[0].size())\n","        #print(data[1].size())\n","        #print(data[1])\n","        loss += svi.step(data[0].view(-1,512*2*2), data[1])\n","    normalizer_train = len(trainloader.dataset) \n","    total_epoch_loss_train = loss / normalizer_train\n","    print(\"Epoch \", j, \" Loss \", total_epoch_loss_train)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["train set saved. len:  50000\n","train path:  /content/gdrive/My Drive/Research/trainset_activaions\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/pyro/primitives.py:406: FutureWarning: The `random_module` primitive is deprecated, and will be removed in a future release. Use `pyro.nn.Module` to create Bayesian modules from `torch.nn.Module` instances.\n","  \"modules from `torch.nn.Module` instances.\", FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch  0  Loss  4176.146883602394\n","Epoch  1  Loss  341.7039912604141\n","Epoch  2  Loss  167.83183834049225\n","Epoch  3  Loss  138.30397884962082\n","Epoch  4  Loss  127.91994792963028\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tlSf19Gbo_Qf","colab_type":"text"},"source":["Load pretrained BNN model"]},{"cell_type":"code","metadata":{"id":"BVZhJ0Y3uKzi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1594685118170,"user_tz":420,"elapsed":614180,"user":{"displayName":"Tamar Geller","photoUrl":"","userId":"04951064580757149713"}},"outputId":"2b495a5e-9e27-40ed-8523-c031010ceabe"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","MODEL_PATH = F\"/content/gdrive/My Drive/Research/trainedBNN\"\n","#TODO"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CwV60nq8ptmE","colab_type":"text"},"source":["## Accuracy Tests"]},{"cell_type":"markdown","metadata":{"id":"zQFWfycK5fwL","colab_type":"text"},"source":["VGG Network Test:"]},{"cell_type":"code","metadata":{"id":"HFKQbOhepxxI","colab_type":"code","colab":{}},"source":["def VGGTest(test_loader,m, sigma):\n","  correct = 0\n","  total = 0\n","  batch_number = 0\n","  with torch.no_grad():\n","      for data in test_loader:\n","          images, labels = data[0].to(device), data[1].to(device)\n","          #print(images.size())\n","          if(sigma>0):\n","            images += torch.normal(m, sigma, size=(100,3,32,32))\n","          outputs, a = net(images)\n","          b, predicted = torch.max(outputs.data, 1)\n","          total += labels.size(0)\n","          predicted = predicted.to(device)\n","          #print((predicted == labels).sum().item()))\n","          correct += (predicted == labels).sum().item()\n","          #print(correct, \"/\", total)\n","          batch_number += 1\n","  accuracy = 100 * correct / total\n","  print('Accuracy of the VGG network on the 10000 test images: %d %%' % (\n","      100 * correct / total))\n","  return accuracy"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pHo6E17EwTw6","colab_type":"text"},"source":["BNN Test When Network Is *Forced* To Predict"]},{"cell_type":"code","metadata":{"id":"5omC1LknuKIb","colab_type":"code","colab":{}},"source":["num_samples = 10\n","def predict(x):\n","    sampled_models = [guide(None, None) for _ in range(num_samples)]\n","    yhats = [model(x).data for model in sampled_models]\n","    mean = torch.mean(torch.stack(yhats), 0)\n","    return np.argmax(mean.numpy(), axis=1)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VbqjfZ61vi5f","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":139},"executionInfo":{"status":"ok","timestamp":1594685221648,"user_tz":420,"elapsed":717644,"user":{"displayName":"Tamar Geller","photoUrl":"","userId":"04951064580757149713"}},"outputId":"895e9594-75d8-4f4a-f2f4-1b65d862450e"},"source":["'''\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n","'''\n","#testset1 = collect_activations(net, device, testloader, 20, 0, 0)\n","from google.colab import drive \n","TEST_PATH = \"/content/gdrive/My Drive/Research/testset_method2.pth\"\n","testset = torch.load(TEST_PATH)\n","print('test set loaded. len: ', len(testset))\n","print('test path: ', TEST_PATH)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n","\n","\n","\n","\n","#testloader1 = torch.utils.data.DataLoader(testset1, batch_size=100, shuffle=False, num_workers=2)\n","print('Prediction when network is forced to predict')\n","correct = 0\n","total = 0\n","for j, data in enumerate(testloader):\n","    images, labels = data\n","    predicted = predict(images.view(-1,512*2*2))\n","    total += labels.size(0)\n","    predicted = torch.Tensor(predicted)\n","    correct += (predicted.eq(labels).sum().item())\n","print('Accuracy of the BNN network on the 10000 test images: %d %%' % (100 * correct / total))\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["test set loaded. len:  10000\n","test path:  /content/gdrive/My Drive/Research/testset_method2.pth\n","Prediction when network is forced to predict\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/pyro/primitives.py:406: FutureWarning: The `random_module` primitive is deprecated, and will be removed in a future release. Use `pyro.nn.Module` to create Bayesian modules from `torch.nn.Module` instances.\n","  \"modules from `torch.nn.Module` instances.\", FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Accuracy of the BNN network on the 10000 test images: 85 %\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Ice4Ek3Opex2","colab_type":"text"},"source":["BNN Test When Network Can Be Undecided"]},{"cell_type":"code","metadata":{"id":"YPIPbj97JIvc","colab_type":"code","colab":{}},"source":["def imshow(img):\n","      img = img / 2 + 0.5     # unnormalize\n","      npimg = img.numpy()\n","      #plt.imshow(npimg,  cmap='gray')\n","      #fig.show(figsize=(1,1))\n","      \n","      fig, ax = plt.subplots(figsize=(1, 1))\n","      ax.imshow(npimg,  cmap='gray', interpolation='nearest')\n","      plt.show()\n","\n","num_samples = 100\n","def give_uncertainities(x):\n","      sampled_models = [guide(None, None) for _ in range(num_samples)]\n","      yhats = [F.log_softmax(model(x.view(-1,512*2*2)).data, 1).detach().numpy() for model in sampled_models]\n","      return np.asarray(yhats)\n","      #mean = torch.mean(torch.stack(yhats), 0)\n","      #return np.argmax(mean, axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fbn7i88pD5kq","colab_type":"code","colab":{}},"source":["def test_batch(test_loader, plot=True):\n","  predicted_for_images = 0\n","  correct_predictions=0 \n","  total = 0\n","  for j, data in enumerate(test_loader):\n","      images, labels = data\n","      y = give_uncertainities(images)        \n","\n","      for i in range(len(labels)):\n","          all_digits_prob = []\n","          highted_something = False\n","          for j in range(len(classes)):\n","              highlight=False\n","              histo = []\n","              histo_exp = []\n","              for z in range(y.shape[0]):\n","                  histo.append(y[z][i][j])\n","                  histo_exp.append(np.exp(y[z][i][j]))\n","              prob = np.percentile(histo_exp, 10) #sampling median probability, sampling 10th percentile to be 50%\n","              if(prob>0.5): #select if network thinks this sample is 50% chance of this being a label\n","                  highlight = True #possibly an answer\n","              all_digits_prob.append(prob)\n","          \n","              if(highlight):\n","                  highted_something = True\n","      \n","          predicted = np.argmax(all_digits_prob)\n","      \n","          if(highted_something):\n","              predicted_for_images+=1\n","              if(labels[i].item()==predicted):\n","                  correct_predictions +=1.0\n","\n","      total+= len(labels)\n","      #print(total)\n","      \n","  if(plot):\n","          print(\"Summary\")\n","          print(\"Total images: \", total, \"10,000\")\n","          print(\"Predicted for: \", predicted_for_images)\n","          print(\"Correct Images \", correct_predictions)\n","          print(\"Accuracy when predicted: \",correct_predictions/predicted_for_images)\n","          good = ((total - predicted_for_images) + correct_predictions)*100 /total\n","          print(\"Good Score\", good)\n","\n","  return total, predicted_for_images, int(correct_predictions)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pBGMfqLtx29m","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1594704232004,"user_tz":420,"elapsed":291,"user":{"displayName":"Tamar Geller","photoUrl":"","userId":"04951064580757149713"}},"outputId":"ee99f098-bb01-49b9-c951-ae4466326403"},"source":["print(sigmas)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[0, 0.5, 1, 1.5, 2]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YGeJr24Tx94l","colab_type":"code","colab":{}},"source":["print(sigma)\n","print(correct)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AHv-m8mPyI4d","colab_type":"code","colab":{}},"source":["print(correct)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q8QATsepZYJN","colab_type":"text"},"source":["ReTrain Bayesian Network on 12th Layer Activations"]},{"cell_type":"code","metadata":{"id":"8QxpftOZM1IR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":507},"executionInfo":{"status":"error","timestamp":1594689702991,"user_tz":420,"elapsed":1868571,"user":{"displayName":"Tamar Geller","photoUrl":"","userId":"04951064580757149713"}},"outputId":"2533b610-8908-4fc5-b36c-fc9a9b57155a"},"source":["import pandas as pd\n","batch_size = 100\n","num_batches = 100\n","target_layer_idx = 20\n","\n","VGG_incorrect = pd.DataFrame(columns=['Sigmas','Total','Predicted','Correct'])\n","\n","\n","#loading CIFAR-10 images\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n","\n","sigmas = [0, 0.5, 1, 1.5, 2]\n","sigma = []\n","total = []\n","predicted = []\n","correct = []\n","\n","for i in sigmas:\n","  correct, incorrect = createPositiveNegativeDatasets(net, device, testloader, target_layer_idx, 0, i)\n","  correct_path = f\"/content/gdrive/My Drive/Research/noise_testsets/correct/noise{i}_correct_testset\"\n","  incorrect_path = f\"/content/gdrive/My Drive/Research/noise_testsets/incorrect/noise{i}_incorrect_testset\"\n","  torch.save(correct,correct_path)\n","  torch.save(incorrect,incorrect_path)\n","  print(\"Saved correct and inccorrect testsets for sigma = \",i)\n","\n","print(\"Done creating test sets\")\n","\n","\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Files already downloaded and verified\n","Saved correct and inccorrect testsets for sigma =  0\n","Saved correct and inccorrect testsets for sigma =  0.5\n","Saved correct and inccorrect testsets for sigma =  1\n","Saved correct and inccorrect testsets for sigma =  1.5\n","Saved correct and inccorrect testsets for sigma =  2\n","Done creating test sets\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/pyro/primitives.py:406: FutureWarning: The `random_module` primitive is deprecated, and will be removed in a future release. Use `pyro.nn.Module` to create Bayesian modules from `torch.nn.Module` instances.\n","  \"modules from `torch.nn.Module` instances.\", FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Summary\n","Total images:  9139 10,000\n","Predicted for:  7804\n","Correct Images  7802.0\n","Accuracy when predicted:  0.9997437211686314\n","Good Score 99.97811576758946\n"],"name":"stdout"},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-27-7405948f85c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m   \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m   \u001b[0mtestloader_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m   \u001b[0mtot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestloader_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m   \u001b[0msigma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m   \u001b[0mtotal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'float' object is not iterable"]}]},{"cell_type":"code","metadata":{"id":"4jNCugGVXzQt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1594710643169,"user_tz":420,"elapsed":5752792,"user":{"displayName":"Tamar Geller","photoUrl":"","userId":"04951064580757149713"}},"outputId":"7af5b963-e92e-42f1-da3e-8a2bde0f7f10"},"source":["sigmas = [0, 0.5, 1, 1.5, 2]\n","sigma = []\n","total = []\n","predicted = []\n","correct1 = []\n","\n","for i in sigmas:\n","  correct_path = f\"/content/gdrive/My Drive/Research/noise_testsets/correct/noise{i}_correct_testset\"\n","  correct = torch.load(correct_path)\n","  testloader_c = torch.utils.data.DataLoader(correct, batch_size=100, shuffle=False, num_workers=2)\n","  tot, pred, cor = test_batch(testloader_c)\n","  sigma.append(i)\n","  total.append(tot)\n","  predicted.append(pred)\n","  correct1.append(cor)\n","  print(\"Tested BNN on VGG correct data for sigma = \",i)\n","\n","\n","for i in sigmas:\n","  incorrect_path = f\"/content/gdrive/My Drive/Research/noise_testsets/incorrect/noise{i}_incorrect_testset\"\n","  incorrect = torch.load(incorrect_path)\n","  testloader_f = torch.utils.data.DataLoader(incorrect, batch_size=100, shuffle=False, num_workers=2)\n","  tot, pred, cor = test_batch(testloader_f)\n","  sigma.append(i)\n","  total.append(tot)\n","  predicted.append(pred)\n","  correct1.append(cor)\n","  print(\"Tested BNN on VGG incorrect data for sigma = \",i)\n","\n","VGG_incorrect['Sigmas'] = sigma\n","VGG_incorrect['Total'] = total\n","VGG_incorrect['Predicted'] = predicted\n","VGG_incorrect['Correct'] = correct1\n","\n","VGG_incorrect_path = F\"/content/gdrive/My Drive/Research/Noise_F1_Data.csv\"\n","\n","VGG_incorrect.to_csv(VGG_incorrect_path)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/pyro/primitives.py:406: FutureWarning: The `random_module` primitive is deprecated, and will be removed in a future release. Use `pyro.nn.Module` to create Bayesian modules from `torch.nn.Module` instances.\n","  \"modules from `torch.nn.Module` instances.\", FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Summary\n","Total images:  9139 10,000\n","Predicted for:  7744\n","Correct Images  7744.0\n","Accuracy when predicted:  1.0\n","Good Score 100.0\n","Tested BNN on VGG correct data for sigma =  0\n","Summary\n","Total images:  2308 10,000\n","Predicted for:  1330\n","Correct Images  1318.0\n","Accuracy when predicted:  0.9909774436090225\n","Good Score 99.48006932409012\n","Tested BNN on VGG correct data for sigma =  0.5\n","Summary\n","Total images:  1035 10,000\n","Predicted for:  995\n","Correct Images  990.0\n","Accuracy when predicted:  0.9949748743718593\n","Good Score 99.51690821256038\n","Tested BNN on VGG correct data for sigma =  1\n","Summary\n","Total images:  1019 10,000\n","Predicted for:  975\n","Correct Images  972.0\n","Accuracy when predicted:  0.9969230769230769\n","Good Score 99.70559371933268\n","Tested BNN on VGG correct data for sigma =  1.5\n","Summary\n","Total images:  1018 10,000\n","Predicted for:  916\n","Correct Images  914.0\n","Accuracy when predicted:  0.9978165938864629\n","Good Score 99.80353634577604\n","Tested BNN on VGG correct data for sigma =  2\n","Summary\n","Total images:  861 10,000\n","Predicted for:  204\n","Correct Images  7.0\n","Accuracy when predicted:  0.03431372549019608\n","Good Score 77.11962833914053\n","Tested BNN on VGG incorrect data for sigma =  0\n","Summary\n","Total images:  7692 10,000\n","Predicted for:  5736\n","Correct Images  2.0\n","Accuracy when predicted:  0.0003486750348675035\n","Good Score 25.45501820072803\n","Tested BNN on VGG incorrect data for sigma =  0.5\n","Summary\n","Total images:  8965 10,000\n","Predicted for:  8721\n","Correct Images  0\n","Accuracy when predicted:  0.0\n","Good Score 2.7216954824316786\n","Tested BNN on VGG incorrect data for sigma =  1\n","Summary\n","Total images:  8981 10,000\n","Predicted for:  8737\n","Correct Images  1.0\n","Accuracy when predicted:  0.00011445576284765938\n","Good Score 2.7279812938425567\n","Tested BNN on VGG incorrect data for sigma =  1.5\n","Summary\n","Total images:  8982 10,000\n","Predicted for:  8075\n","Correct Images  0\n","Accuracy when predicted:  0.0\n","Good Score 10.097973725228234\n","Tested BNN on VGG incorrect data for sigma =  2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bDePvwm0yYJB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":190},"executionInfo":{"status":"ok","timestamp":1594704817441,"user_tz":420,"elapsed":107257,"user":{"displayName":"Tamar Geller","photoUrl":"","userId":"04951064580757149713"}},"outputId":"15609c88-0098-419e-aead-37e30845e77e"},"source":["incorrect_path = \"/content/gdrive/My Drive/Research/noise_testsets/incorrect/noise0_incorrect_testset\"\n","incorrect = torch.load(incorrect_path)\n","testloader_f = torch.utils.data.DataLoader(incorrect, batch_size=100, shuffle=False, num_workers=2)\n","tot, pred, cor = test_batch(testloader_f)\n","print(cor)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/pyro/primitives.py:406: FutureWarning: The `random_module` primitive is deprecated, and will be removed in a future release. Use `pyro.nn.Module` to create Bayesian modules from `torch.nn.Module` instances.\n","  \"modules from `torch.nn.Module` instances.\", FutureWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["Summary\n","Total images:  861 10,000\n","Predicted for:  198\n","Correct Images  6.0\n","Accuracy when predicted:  0.030303030303030304\n","Good Score 77.70034843205575\n","6\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5zMICBn5uk6X","colab_type":"code","colab":{}},"source":["\n","'''\n","yVGG = []\n","\n","#testing VGG network with varying levels of noise\n","for i in sigmas:\n","  x = VGGTest(testloader, 0, i)\n","  yVGG.append(x)\n","  print(i)\n","\n","print(yVGG)\n","\n","#Loading activations datasets\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","TEST_PATH = F\"/content/gdrive/My Drive/Research/testset\"\n","\n","testset = torch.load(TEST_PATH, map_location='cpu' )\n","testloader1 = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=2)\n","'''\n","\n","yBNN = []\n","for i in sigmas:\n","  dataset = collect_activations(net, device, testloader, target_layer_idx, 0, i)\n","  DATASET_PATH = f\"/content/gdrive/My Drive/Research/testset_layer12_activations_sigma{i}.pth\"\n","  torch.save(dataset, DATASET_PATH)\n","  test_loader0 = torch.utils.data.DataLoader(dataset, batch_size=100, shuffle=False, num_workers=2)\n","  print(f\"sigma: {i}\")\n","  bnn = test_batch(test_loader0)\n","  yBNN.append(bnn)\n","\n","\n","print(f\"Results for VGG, BNN Network trained on Layer {target_layer_idc} Activations with Threshold 0.5 sampling 10th percentile\")\n","print(\"BNN Good Scores\", yBNN)\n","yVGG = [91.39, 23.46, 10.29, 10.11, 10.1]\n","print(\"VGG Good Scores\", yVGG)\n","\n","\n","print('Done with testsets')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nd81b7SA6H78","colab_type":"text"},"source":["## Graphs"]},{"cell_type":"code","metadata":{"id":"OKqI29lA6Jk-","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","\n","\n","yVGG = [91.39, 23.46, 10.29, 10.11, 10.1]\n","yBNN = [91, 100.0, 100.0, 99.64, 99.95]\n","sigma = [0, 0.5, 1, 1.5, 2]\n","\n","\n","fig = plt.figure(1, figsize=(15, 7), dpi=95)\n","#plt.subplot(231)\n","#plt.ylim(ylow, ylim)\n","plt.plot( sigma, yVGG, label = 'VGG Good Score')\n","plt.plot(sigma, yBNN, label = 'BNN Good Score')\n","plt.legend(frameon = False)\n","plt.xlabel('Sigma')\n","plt.ylabel('Good score %')\n","plt.title('Good Scores When Given Random Noise')\n","plt.show()\n","\n","yVGG = [91.39, 12.45, 10.45, 10.21]\n","yBNN = [91, 99.65, 99.99, 100]\n","eps = [0, 1, 2, 3]\n","\n","fig = plt.figure(1, figsize=(15, 7), dpi=95)\n","#plt.subplot(231)\n","#plt.ylim(ylow, ylim)\n","plt.plot( eps, yVGG, label = 'VGG Good Score')\n","plt.plot(eps, yBNN, label = 'BNN Good Score')\n","plt.legend(frameon = False)\n","plt.xlabel('Epsilon')\n","plt.ylabel('Good score %')\n","plt.title('Good Scores When Adverserially Attacked')\n","plt.show()\n","\n","\n","\n"],"execution_count":null,"outputs":[]}]}